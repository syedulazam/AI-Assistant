# -*- coding: utf-8 -*-
"""Copy of Athena AI.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NwAujaj9rsgLVOAooHmpO_Pd_wgjZNy4

# Athena AI

---

### Import  the Needed Packages
"""

!pip list

import warnings
warnings.filterwarnings('ignore')

!pip install datasets
!pip install openai
!pip install pinecone
!pip install tqdm
!pip install pandas
# !pip install "pinecone-client[grpc]"
!pip install utils

# !pip uninstall pinecone-client -y

# pip install "pinecone-client[grpc]"

# !pip install utils

class Utils:
    def get_config(self):
        with open('config.json', 'r') as config_file:
            return json.load(config_file)

    def get_pinecone_api_key(self):
        with open('config.json', 'r') as config_file:
            config = json.load(config_file)
            return config['PINECONE_API_KEY']

    def get_openai_api_key(self):
        with open('config.json', 'r') as config_file:
            config = json.load(config_file)
            return config['OPENAI_API_KEY']

from datasets import load_dataset
import json
from openai import OpenAI
import pinecone
from tqdm import tqdm
from pinecone import Pinecone, ServerlessSpec

import ast
import os
import pandas as pd

# get api key
utils = Utils()
PINECONE_API_KEY = utils.get_pinecone_api_key()
OPENAI_API_KEY = utils.get_openai_api_key()

print((OPENAI_API_KEY))  # This should print your OpenAI API key
print((PINECONE_API_KEY))  # This should print your Pinecone API key

pinecone = Pinecone(api_key=PINECONE_API_KEY)

# pinecone.create_index(
#     name="aai",
#     dimension=1536,
#     metric="cosine",
#     spec=ServerlessSpec(
#         cloud="aws",
#         region="us-east-1"
#     )
# )

index = pinecone.Index("aai")

!wget -q -O lesson2-wiki.csv.zip "https://www.dropbox.com/scl/fi/yxzmsrv2sgl249zcspeqb/lesson2-wiki.csv.zip?rlkey=paehnoxjl3s5x53d1bedt4pmc&dl=0"
!unzip lesson2-wiki.csv.zip

max_articles_num = 500
df = pd.read_csv('/content/greys_anatomy.csv', nrows=max_articles_num)
df.head()

"""### Prepare the Embeddings and Upsert to Pinecone"""

# from pinecone.grpc import PineconeGRPC as Pinecone

prepped = []

for i, row in tqdm(df.iterrows(), total=df.shape[0]):
    prepped.append({
        'id': str(i),
        'values': ast.literal_eval(row['embedding']),
        'metadata': {'chunk': row['metadata'], 'text': row['text']}
    })
    if len(prepped) >= 100:
        index.upsert(vectors=prepped)
        prepped = []

# Upsert remaining vectors if any
if prepped:
    index.upsert(vectors=prepped)

index.describe_index_stats()

"""### Connect to OpenAI"""

openai_client = OpenAI(api_key=OPENAI_API_KEY)

!pip install langchain-community

!pip install -qU \
  langchain==0.1.1 \
  langchain-community==0.0.13 \
  openai==0.27.7 \
  tiktoken==0.4.0 \
  pinecone-client==3.1.0 \
  pinecone-datasets==0.7.0 \
  pinecone-notebooks==0.1.1

OPENAI_API_KEY = utils.get_openai_api_key()
openai_client = OpenAI(api_key=OPENAI_API_KEY)

def get_embeddings(articles, model="text-embedding-ada-002"):
   return openai_client.embeddings.create(input = articles, model=model)

from langchain.embeddings.openai import OpenAIEmbeddings

# get openai api key from platform.openai.com
OPENAI_API_KEY = os.getenv('OPENAI_API_KEY') or 'OPENAI_API_KEY'

model_name = 'text-embedding-ada-002'

embed = OpenAIEmbeddings(
    model=model_name,
    openai_api_key=OPENAI_API_KEY
)

"""### Run Your Query"""

# # Set up Langchain Pinecone vector store
# from langchain.vectorstores import Pinecone

# text_field = "text"

# vectorstore = Pinecone(
#     index, embed.embed_query, text_field
# )

query = "Explain the functions of the top three parts of a cell"

embed = get_embeddings([query])
res = index.query(vector=embed.data[0].embedding, top_k=3, include_metadata=True)
text = [r['metadata']['text'] for r in res['matches']]
print('\n'.join(text))

"""### Build the Prompt"""

query = "write an article titled: Explain the functions of the top three parts of a cell"
embed = get_embeddings([query])
res = index.query(vector=embed.data[0].embedding, top_k=3, include_metadata=True)

contexts = [
    x['metadata']['text'] for x in res['matches']
]

prompt_start = (
    "Answer the question based on the context below.\n\n"+
    "Context:\n"
)

prompt_end = (
    f"\n\nQuestion: {query}\nAnswer:"
)

prompt = (
    prompt_start + "\n\n---\n\n".join(contexts) +
    prompt_end
)

print(prompt)

"""### Get the Summary"""

# res = openai_client.completions.create(
#     model="gpt-3.5-turbo-instruct",
#     prompt=prompt,
#     temperature=0,
#     max_tokens=636,
#     top_p=1,
#     frequency_penalty=0,
#     presence_penalty=0,
#     stop=None
# )
# print('-' * 80)
# print(res.choices[0].text)

res = openai_client.chat.completions.create(
    model="gpt-4o-mini-2024-07-18",
    messages=[{"role": "user", "content": prompt}],
    temperature=0,
    max_tokens=636,
    top_p=1,
    frequency_penalty=0,
    presence_penalty=0
)

print('-' * 80)
print(res.choices[0].message.content)

#GPT 4O mini
def get_query(query):
    get_embeddings([query])
    res = index.query(vector=embed.data[0].embedding, top_k=3, include_metadata=True)
    contexts = [
        x['metadata']['text'] for x in res['matches']
    ]
    prompt_start = (
        "Answer the question based on the context below.\n\n" +
        "Context:\n"
    )
    prompt_end = (
        f"\n\nQuestion: {query}\nAnswer:"
    )
    prompt = (
        prompt_start + "\n\n---\n\n".join(contexts) +
        prompt_end
    )
    print(prompt)
    return prompt

def predict(query):
    prompt = get_query(query)
    completion = openai_client.chat.completions.create(
        model="gpt-4o-mini-2024-07-18",
        messages=[{"role": "user", "content": prompt}],
        temperature=0,
        max_tokens=636,
        top_p=1,
        frequency_penalty=0,
        presence_penalty=0
    )
    #print(completion.choices[0].message.content)
    return completion.choices[0].message.content

!pip install gradio

css = """
body {
    font-family: 'Poppins', sans-serif !important;
    background: linear-gradient(to right, #ece9e6, #ffffff) !important;
    color: #333 !important;
    margin: 0 !important;
    padding: 0 !important;
}

h1 {
    text-align: center !important;
    font-size: 2.5rem !important;
    color: #2c3e50 !important;
    font-weight: 600 !important;
    margin-bottom: 20px !important;
}

.gradio-container {
    border-radius: 12px !important;
    box-shadow: 0px 10px 30px rgba(0, 0, 0, 0.15) !important;
    padding: 25px !important;
    background: #ffffff !important;
    max-width: 800px !important;
    margin: 30px auto !important;
    font-size: 16px !important;
    transition: all 0.3s ease-in-out !important;
}

.gradio-container:hover {
    transform: scale(1.02) !important;
}

.chat-container {
    display: flex !important;
    flex-direction: column !important;
    gap: 15px !important;
}

.chat-bubble {
    display: flex !important;
    flex-direction: row !important;
    align-items: center !important;
}

.chat-bubble.user {
    justify-content: flex-end !important;
}

.chat-bubble.ai {
    justify-content: flex-start !important;
}

.chat-bubble .message {
    max-width: 65% !important;
    padding: 12px 15px !important;
    border-radius: 18px !important;
    box-shadow: 0px 5px 15px rgba(0, 0, 0, 0.1) !important;
    font-size: 16px !important;
    line-height: 1.5 !important;
    transition: all 0.3s ease-in-out !important;
}

.chat-bubble.user .message {
    background: linear-gradient(to right, #3498db, #2980b9) !important;
    color: white !important;
}

.chat-bubble.ai .message {
    background: linear-gradient(to right, #e0e0e0, #d3d3d3) !important;
    color: #333 !important;
}

textarea {
    font-family: 'Poppins', sans-serif !important;
    font-size: 16px !important;
    border: 2px solid #dcdde1 !important;
    border-radius: 8px !important;
    padding: 12px !important;
    margin-bottom: 15px !important;
    width: 100% !important;
    transition: all 0.3s ease-in-out !important;
}

textarea:focus {
    border-color: #3498db !important;
    outline: none !important;
    box-shadow: 0px 0px 8px rgba(52, 152, 219, 0.5) !important;
}

button {
    background: linear-gradient(to right, #2c3e50, #4b6584) !important;
    color: #ffffff !important;
    border: none !important;
    padding: 12px 25px !important;
    border-radius: 8px !important;
    cursor: pointer !important;
    font-size: 16px !important;
    font-weight: bold !important;
    text-transform: uppercase !important;
    transition: all 0.3s ease-in-out !important;
}

button:hover {
    background: linear-gradient(to right, #4b6584, #2c3e50) !important;
    transform: scale(1.05) !important;
}
"""

html_template = """
<div class="chat-container">
    {% for query, response in chat_history %}
    <div class="chat-bubble user">
        <div class="message">{{ query }}</div>
    </div>
    <div class="chat-bubble ai">
        <div class="message">{{ response }}</div>
    </div>
    {% endfor %}
</div>
"""

import gradio as gr
import re

def format_response(text):
    """Formats the GPT response for better readability in the chat UI."""
    lines = text.split("\n")
    formatted_text = []

    for line in lines:
        line = line.strip()
        if re.match(r"^[-â€¢]\s", line) or re.match(r"^\d+\.\s", line):  # Bullet points & numbered lists
            formatted_text.append(f"<br>{line}")
        elif line:  # Normal text paragraphs
            formatted_text.append(line)

    return "<br>".join(formatted_text)

def render_chat_html(chat_history):
    html_content = "<div class='chat-container'>"
    for query, response in chat_history:
        formatted_response = format_response(response)  # Apply formatting
        html_content += f"""
        <div class='chat-bubble user'>
            <div class='message'>{query}</div>
        </div>
        <div class='chat-bubble ai'>
            <div class='message'>{formatted_response}</div>
        </div>
        """
    html_content += "</div>"
    return html_content

def chat(query, chat_history):
    response = predict(query)
    chat_history.append((query, response))
    return chat_history, render_chat_html(chat_history), ""

with gr.Blocks(css=css) as interface:
    with gr.Column():
        gr.Markdown("<h1>Athena AI</h1>")
        chat_history = gr.State([])
        input_text = gr.Text(
            lines=2,
            placeholder="Enter your query...",
            interactive=True
        )
        submit_button = gr.Button("Submit")
        output_display = gr.HTML("")

    submit_button.click(chat, [input_text, chat_history], [chat_history, output_display, input_text])
    input_text.submit(chat, [input_text, chat_history], [chat_history, output_display, input_text])  # Pressing Enter submits

interface.launch()

predict("Explain the functions of the top three parts of a cell")